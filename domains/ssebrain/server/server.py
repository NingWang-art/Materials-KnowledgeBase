"""
SSEBrain Structured Searché—®ç­”ç³»ç»Ÿ MCP Server
æä¾›åŸºäºç»“æ„åŒ–æ•°æ®åº“æ£€ç´¢çš„SSEBrainæ–‡çŒ®çŸ¥è¯†é—®ç­”æœåŠ¡
"""
import argparse
import logging
import json
import asyncio
import requests
from typing import List, Dict, Tuple, Optional
try:
    from typing_extensions import TypedDict
except ImportError:
    from typing import TypedDict
from pathlib import Path
from datetime import datetime
import sys
from anyio import to_thread

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path("/home/knowledge_base")))

from dp.agent.server import CalculationMCPServer

# ä»coreå¯¼å…¥ç”Ÿæˆå™¨
from core.generator import DeepSeekGenerator

# å¯¼å…¥é€šç”¨æ¨¡å—
from common.constants import StatusCode
from tools.utils import (
    generate_literature_summaries_parallel,
    call_llm_api
)

# å¯¼å…¥ssebrainç‰¹å®šæ¨¡å—
try:
    from .config import (
        SERVER_HOST, SERVER_PORT, LOG_LEVEL, MAX_WORKERS, LLM_API_TIMEOUT, LLM_MAX_RETRIES,
        DB_NAME, DEEPSEEK_CONFIG
    )
    from .prompts import (
        LITERATURE_SUMMARY_SYSTEM_PROMPT,
        get_literature_summary_user_prompt,
        DATABASE_QUERY_SYSTEM_PROMPT,
        get_database_query_user_prompt
    )
    from .utils import read_literature_fulltext
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨ç»å¯¹å¯¼å…¥ï¼ˆç›´æ¥è¿è¡Œserver.pyæ—¶ï¼‰
    from domains.ssebrain.server.config import (
        SERVER_HOST, SERVER_PORT, LOG_LEVEL, MAX_WORKERS, LLM_API_TIMEOUT, LLM_MAX_RETRIES,
        DB_NAME, DEEPSEEK_CONFIG
    )
    from domains.ssebrain.server.prompts import (
        LITERATURE_SUMMARY_SYSTEM_PROMPT,
        get_literature_summary_user_prompt,
        DATABASE_QUERY_SYSTEM_PROMPT,
        get_database_query_user_prompt
    )
    from domains.ssebrain.server.utils import read_literature_fulltext

# å¯¼å…¥DatabaseManager
# æ·»åŠ domainsè·¯å¾„ä»¥æ”¯æŒå¯¼å…¥
sys.path.insert(0, str(Path("/home/knowledge_base/domains")))
# ä½¿ç”¨ssebrain_agentä¸­çš„DatabaseManagerï¼Œå®ƒæ”¯æŒsolid_state_electrolyte_db
from ssebrain.ssebrain_agent.tools.database import DatabaseManager

MAX_FULLTEXT_SUMMARIES = 20  # deep researchæ—¶æœ€å¤šå¤„ç†çš„å…¨æ–‡æ–‡çŒ®æ•°

# === ARG PARSING ===
def parse_args():
    parser = argparse.ArgumentParser(description="SSEBrain Structured Search MCP Server")
    parser.add_argument('--port', type=int, default=SERVER_PORT, 
                       help=f'Server port (default: {SERVER_PORT})')
    parser.add_argument('--host', default=SERVER_HOST, 
                       help=f'Server host (default: {SERVER_HOST})')
    parser.add_argument('--log-level', default=LOG_LEVEL,
                        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                        help=f'Logging level (default: {LOG_LEVEL})')
    try:
        return parser.parse_args()
    except SystemExit:
        class Args:
            port = SERVER_PORT
            host = SERVER_HOST
            log_level = LOG_LEVEL
        return Args()

# === OUTPUT TYPE ===
class QueryResult(TypedDict):
    summaries: List[str]           # æ–‡çŒ®æ€»ç»“åˆ—è¡¨
    code: int                      # çŠ¶æ€ç ï¼š0=æ­£å¸¸ï¼Œé0=å¼‚å¸¸


# === STRUCTURED SEARCH SYSTEM INITIALIZATION ===
class StructuredSearchSystem:
    """ç»“æ„åŒ–æ£€ç´¢ç³»ç»Ÿå•ä¾‹"""
    _instance = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not self._initialized:
            logging.info("åˆå§‹åŒ–ç»“æ„åŒ–æ£€ç´¢ç³»ç»Ÿ...")
            
            # åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
            self.db_manager = DatabaseManager(DB_NAME)
            # å¼‚æ­¥åˆå§‹åŒ–æ•°æ®åº“ï¼ˆè·å–è¡¨ç»“æ„ï¼‰
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            loop.run_until_complete(self.db_manager.async_init())
            loop.close()
            
            # åˆ›å»ºç”Ÿæˆå™¨ï¼šç”¨äºæ–‡çŒ®æ€»ç»“å’ŒæŸ¥è¯¢è½¬æ¢
            self.summary_generator = DeepSeekGenerator(**DEEPSEEK_CONFIG)
            self.query_generator = DeepSeekGenerator(**DEEPSEEK_CONFIG)
            
            logging.info("ç»“æ„åŒ–æ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
            
            StructuredSearchSystem._initialized = True
    
    def _convert_query_to_filters(self, query_description: str) -> Dict:
        """
        ä½¿ç”¨LLMå°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºç»“æ„åŒ–filters
        
        Args:
            query_description: è‡ªç„¶è¯­è¨€æŸ¥è¯¢æè¿°
            
        Returns:
            ç»“æ„åŒ–filterså­—å…¸ï¼Œå¦‚æœè½¬æ¢å¤±è´¥è¿”å›None
        """
        try:
            system_prompt = DATABASE_QUERY_SYSTEM_PROMPT
            user_prompt = get_database_query_user_prompt(query_description)
            
            # è°ƒç”¨LLMç”Ÿæˆfilters JSON
            headers = {
                "Authorization": f"Bearer {self.query_generator.api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": self.query_generator.model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt + "\n\nè¯·ç›´æ¥è¿”å›JSONæ ¼å¼çš„filtersç»“æ„ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šæ–‡å­—ã€‚"}
                ],
                "temperature": 0.1  # ä½¿ç”¨è¾ƒä½æ¸©åº¦ä»¥ç¡®ä¿ç»“æ„åŒ–è¾“å‡º
            }
            
            response = requests.post(
                self.query_generator.api_url, 
                headers=headers, 
                json=data, 
                timeout=60
            )
            response.raise_for_status()
            
            result = response.json()
            filters_text = result['choices'][0]['message']['content']
            
            # å°è¯•æå–JSONï¼ˆå¯èƒ½åŒ…å«markdownä»£ç å—ï¼‰
            filters_text = filters_text.strip()
            if filters_text.startswith("```"):
                # ç§»é™¤markdownä»£ç å—æ ‡è®°
                filters_text = filters_text.split("```")[1]
                if filters_text.startswith("json"):
                    filters_text = filters_text[4:]
                filters_text = filters_text.strip()
            
            filters = json.loads(filters_text)
            logging.info(f"æˆåŠŸè½¬æ¢æŸ¥è¯¢ä¸ºfilters: {filters}")
            return filters
            
        except Exception as e:
            logging.error(f"è½¬æ¢æŸ¥è¯¢ä¸ºfilterså¤±è´¥: {e}", exc_info=True)
            return None
    
    async def _query_database(self, filters: Dict, table_name: str = "526kq03") -> Tuple[List[str], Dict[str, Dict]]:
        """
        æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢ï¼Œè¿”å›è®ºæ–‡DOIåˆ—è¡¨å’Œå…ƒæ•°æ®ä¿¡æ¯
        
        Args:
            filters: ç»“æ„åŒ–filters
            table_name: è¦æŸ¥è¯¢çš„è¡¨åï¼ˆé»˜è®¤ä½¿ç”¨è®ºæ–‡å…ƒæ•°æ®è¡¨526kq03ï¼‰
            
        Returns:
            (DOIåˆ—è¡¨, DOIåˆ°å…ƒæ•°æ®çš„æ˜ å°„å­—å…¸)
        """
        try:
            query_table = self.db_manager.init_query_table()
            filters_json = json.dumps(filters)
            
            result = await query_table(
                table_name=table_name,
                filters_json=filters_json,
                selected_fields=None,
                page=1,
                page_size=100  # é™åˆ¶æœ€å¤š100æ¡ç»“æœ
            )
            
            if 'error' in result:
                logging.warning(f"æ•°æ®åº“æŸ¥è¯¢é”™è¯¯: {result['error']}")
                return [], {}
            
            dois = result.get('papers', [])
            logging.info(f"æ•°æ®åº“æŸ¥è¯¢å®Œæˆï¼Œæ‰¾åˆ° {len(dois)} ç¯‡è®ºæ–‡")
            
            # å¦‚æœæœ‰DOIåˆ—è¡¨ï¼ŒæŸ¥è¯¢è®ºæ–‡å…ƒæ•°æ®è¡¨è·å–è¯¦ç»†ä¿¡æ¯
            metadata_dict = {}
            if dois:
                # æŸ¥è¯¢è®ºæ–‡å…ƒæ•°æ®è¡¨ (526kq03) - å¦‚æœä¸»æŸ¥è¯¢è¡¨å°±æ˜¯å…ƒæ•°æ®è¡¨ï¼Œåˆ™ç›´æ¥ä½¿ç”¨resultä¸­çš„result
                if result.get('result'):
                    # å¦‚æœä¸»æŸ¥è¯¢å·²ç»è¿”å›äº†å…ƒæ•°æ®ï¼Œç›´æ¥ä½¿ç”¨
                    for paper in result['result']:
                        doi = paper.get('doi')
                        if doi:
                            metadata_dict[doi] = paper
                else:
                    # å¦åˆ™å•ç‹¬æŸ¥è¯¢å…ƒæ•°æ®è¡¨
                    paper_metadata_filters = {
                        "type": 1,
                        "field": "doi",
                        "operator": "in",
                        "value": dois[:100]  # é™åˆ¶æœ€å¤š100ä¸ªDOI
                    }
                    metadata_result = await query_table(
                        table_name="526kq03",
                        filters_json=json.dumps(paper_metadata_filters),
                        selected_fields=None,
                        page=1,
                        page_size=100
                    )
                    
                    if 'error' not in metadata_result and metadata_result.get('result'):
                        for paper in metadata_result['result']:
                            doi = paper.get('doi')
                            if doi:
                                metadata_dict[doi] = paper
                
                logging.info(f"è·å–åˆ° {len(metadata_dict)} ç¯‡è®ºæ–‡çš„å…ƒæ•°æ®")
            
            return dois, metadata_dict
            
        except Exception as e:
            logging.error(f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}", exc_info=True)
            return [], {}
    
    def _generate_metadata_summary(self, doi: str, metadata: Dict, query_description: str) -> str:
        """
        åŸºäºæ•°æ®åº“å…ƒæ•°æ®ç”Ÿæˆæ–‡çŒ®ç®€è¦æ€»ç»“
        
        Args:
            doi: æ–‡çŒ®DOI
            metadata: è®ºæ–‡å…ƒæ•°æ®å­—å…¸
            query_description: ç”¨æˆ·æŸ¥è¯¢æè¿°
            
        Returns:
            åŸºäºå…ƒæ•°æ®çš„ç®€è¦æ€»ç»“æ–‡æœ¬
        """
        try:
            # æå–å…ƒæ•°æ®ä¿¡æ¯
            title = metadata.get('title', metadata.get('Title', 'æœªçŸ¥æ ‡é¢˜'))
            authors = metadata.get('authors', metadata.get('Authors', metadata.get('author', 'æœªçŸ¥ä½œè€…')))
            journal = metadata.get('journal', metadata.get('Journal', metadata.get('journal_name', 'æœªçŸ¥æœŸåˆŠ')))
            year = metadata.get('year', metadata.get('Year', metadata.get('publication_year', '')))
            abstract = metadata.get('abstract', metadata.get('Abstract', ''))
            
            # æ„å»ºç®€è¦æ€»ç»“
            summary_parts = [
                f"**æ–‡çŒ®ä¿¡æ¯ï¼ˆä»…æ•°æ®åº“å…ƒæ•°æ®ï¼Œæ— å…¨æ–‡ï¼‰**",
                f"",
                f"**æ ‡é¢˜**: {title}",
                f"**DOI**: {doi}",
            ]
            
            if authors and authors != 'æœªçŸ¥ä½œè€…':
                summary_parts.append(f"**ä½œè€…**: {authors}")
            
            if journal and journal != 'æœªçŸ¥æœŸåˆŠ':
                summary_parts.append(f"**æœŸåˆŠ**: {journal}")
            
            if year:
                summary_parts.append(f"**å‘è¡¨å¹´ä»½**: {year}")
            
            if abstract:
                # é™åˆ¶æ‘˜è¦é•¿åº¦
                abstract_text = abstract[:500] + "..." if len(abstract) > 500 else abstract
                summary_parts.append(f"")
                summary_parts.append(f"**æ‘˜è¦**: {abstract_text}")
            
            summary_parts.append(f"")
            summary_parts.append(f"**è¯´æ˜**: æ­¤æ–‡çŒ®åœ¨æ•°æ®åº“ä¸­æ— å…¨æ–‡å†…å®¹ï¼Œä»¥ä¸Šä¿¡æ¯æ¥è‡ªæ•°æ®åº“å…ƒæ•°æ®ã€‚")
            
            return "\n".join(summary_parts)
            
        except Exception as e:
            logging.error(f"ç”Ÿæˆå…ƒæ•°æ®æ€»ç»“å¤±è´¥ {doi}: {e}", exc_info=True)
            return f"**æ–‡çŒ®ä¿¡æ¯ï¼ˆä»…æ•°æ®åº“å…ƒæ•°æ®ï¼‰**\n\n**DOI**: {doi}\n\n**è¯´æ˜**: æ­¤æ–‡çŒ®åœ¨æ•°æ®åº“ä¸­æ— å…¨æ–‡å†…å®¹ï¼Œä»…æä¾›DOIä¿¡æ¯ã€‚"
    
    def query(self, query_description: str) -> QueryResult:
        """
        æ‰§è¡Œç»“æ„åŒ–æ£€ç´¢æŸ¥è¯¢æµç¨‹ï¼š
        1. å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºç»“æ„åŒ–filters
        2. æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢è·å–è®ºæ–‡DOIåˆ—è¡¨å’Œå…ƒæ•°æ®
        3. æ£€æŸ¥å“ªäº›DOIæœ‰å…¨æ–‡
        4. åªå¯¹æœ‰å…¨æ–‡çš„è¿›è¡Œdeep researchï¼ˆç”Ÿæˆè¯¦ç»†æ€»ç»“ï¼‰
        5. å¯¹æ— å…¨æ–‡çš„ä½¿ç”¨å…ƒæ•°æ®ç”Ÿæˆç®€è¦æ¡ç›®
        6. è¿”å›æ–‡çŒ®æ€»ç»“åˆ—è¡¨ï¼ˆæ±‡æ€»ç”±agentå®Œæˆï¼‰
        
        Returns:
            QueryResultåŒ…å«summarieså’Œcode
            summaries: æ–‡çŒ®æ€»ç»“æ–‡æœ¬åˆ—è¡¨ï¼ˆåŒ…å«è¯¦ç»†æ€»ç»“å’Œå…ƒæ•°æ®æ¡ç›®ï¼‰
            code: ä½¿ç”¨StatusCodeå¸¸é‡å®šä¹‰çš„çŠ¶æ€ç 
        """
        try:
            query_start = datetime.now()
            logging.info(f"å¼€å§‹å¤„ç†æŸ¥è¯¢: {query_description}")
            
            # æ­¥éª¤1: å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºç»“æ„åŒ–filters
            logging.info("æ­¥éª¤1: è½¬æ¢è‡ªç„¶è¯­è¨€æŸ¥è¯¢ä¸ºç»“æ„åŒ–filters...")
            filters = self._convert_query_to_filters(query_description)
            
            if not filters:
                logging.warning("æ— æ³•è½¬æ¢æŸ¥è¯¢ä¸ºç»“æ„åŒ–filters")
                return {
                    "summaries": [],
                    "code": StatusCode.OTHER_ERROR
                }
            
            # æ­¥éª¤2: æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢è·å–DOIåˆ—è¡¨å’Œå…ƒæ•°æ®
            logging.info("æ­¥éª¤2: æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢...")
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            dois, metadata_dict = loop.run_until_complete(self._query_database(filters))
            loop.close()
            
            if not dois:
                logging.warning("æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡")
                return {
                    "summaries": [],
                    "code": StatusCode.NO_RESULTS
                }
            
            logging.info(f"æ‰¾åˆ° {len(dois)} ç¯‡ç›¸å…³è®ºæ–‡: {dois[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª
            
            # æ­¥éª¤3: æ£€æŸ¥å“ªäº›DOIæœ‰å…¨æ–‡ï¼ˆåªå¯¹æœ‰å…¨æ–‡çš„è¿›è¡Œdeep researchï¼‰
            logging.info("æ­¥éª¤3: æ£€æŸ¥æ–‡çŒ®å…¨æ–‡å¯ç”¨æ€§...")
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            # æ‰¹é‡æ£€æŸ¥å…¨æ–‡å¯ç”¨æ€§
            async def check_fulltext_availability(doi: str) -> tuple:
                """æ£€æŸ¥å•ä¸ªDOIæ˜¯å¦æœ‰å…¨æ–‡"""
                fulltext = await read_literature_fulltext(doi, self.db_manager)
                return (doi, bool(fulltext))
            
            # å¹¶è¡Œæ£€æŸ¥æ‰€æœ‰DOIçš„å…¨æ–‡å¯ç”¨æ€§
            check_tasks = [check_fulltext_availability(doi) for doi in dois]
            check_results = loop.run_until_complete(asyncio.gather(*check_tasks))
            loop.close()
            
            # åˆ†ç¦»æœ‰å…¨æ–‡å’Œæ— å…¨æ–‡çš„DOI
            dois_with_fulltext = [doi for doi, has_fulltext in check_results if has_fulltext]
            dois_without_fulltext = [doi for doi, has_fulltext in check_results if not has_fulltext]
            
            logging.info(f"å…¨æ–‡å¯ç”¨æ€§æ£€æŸ¥å®Œæˆ: {len(dois_with_fulltext)} ç¯‡æœ‰å…¨æ–‡, {len(dois_without_fulltext)} ç¯‡æ— å…¨æ–‡")
            
            summary_texts = []
            
            # æ­¥éª¤4: åªå¯¹æœ‰å…¨æ–‡çš„DOIè¿›è¡Œå¹¶è¡Œè¯»å–å’Œæ€»ç»“ç”Ÿæˆ
            if dois_with_fulltext:
                if len(dois_with_fulltext) > MAX_FULLTEXT_SUMMARIES:
                    logging.info(
                        f"é™åˆ¶æœ‰å…¨æ–‡æ–‡çŒ®æ•°ä¸º {MAX_FULLTEXT_SUMMARIES} (åŸæœ‰ {len(dois_with_fulltext)} ç¯‡)"
                    )
                    dois_with_fulltext = dois_with_fulltext[:MAX_FULLTEXT_SUMMARIES]
                logging.info(f"æ­¥éª¤4: å¯¹ {len(dois_with_fulltext)} ç¯‡æœ‰å…¨æ–‡çš„æ–‡çŒ®è¿›è¡Œæ€»ç»“ç”Ÿæˆ...")
                
                # åˆ›å»ºé€‚é…å‡½æ•°ï¼šå°†DOIè½¬æ¢ä¸ºfile_idæ ¼å¼ï¼ˆç”¨äºgenerate_literature_summaries_parallelï¼‰
                def read_fulltext_by_doi(doi: str) -> str:
                    """é€‚é…å‡½æ•°ï¼šåŒæ­¥è¯»å–DOIå¯¹åº”çš„å…¨æ–‡"""
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        fulltext = loop.run_until_complete(
                            read_literature_fulltext(doi, self.db_manager)
                        )
                        return fulltext
                    finally:
                        loop.close()
                
                # ä½¿ç”¨generate_literature_summaries_parallelç”Ÿæˆæ€»ç»“
                # æ³¨æ„ï¼šè¿™é‡Œåªä½¿ç”¨æœ‰å…¨æ–‡çš„DOI
                literature_summaries = generate_literature_summaries_parallel(
                    file_ids=dois_with_fulltext,  # åªä½¿ç”¨æœ‰å…¨æ–‡çš„DOI
                    question=query_description,
                    generator=self.summary_generator,
                    system_prompt=LITERATURE_SUMMARY_SYSTEM_PROMPT,
                    get_user_prompt_func=get_literature_summary_user_prompt,
                    read_fulltext_func=read_fulltext_by_doi,
                    max_workers=MAX_WORKERS,
                    timeout=LLM_API_TIMEOUT,
                    max_retries=LLM_MAX_RETRIES
                )
                
                if literature_summaries:
                    summary_texts.extend([summary['summary'] for summary in literature_summaries])
            
            # æ­¥éª¤5: å¯¹äºæ— å…¨æ–‡çš„æ–‡çŒ®ï¼Œä½¿ç”¨æ•°æ®åº“å…ƒæ•°æ®ç”Ÿæˆç®€è¦æ¡ç›®
            if dois_without_fulltext:
                logging.info(f"æ­¥éª¤5: ä¸º {len(dois_without_fulltext)} ç¯‡æ— å…¨æ–‡çš„æ–‡çŒ®ç”Ÿæˆå…ƒæ•°æ®æ¡ç›®...")
                
                for doi in dois_without_fulltext:
                    metadata = metadata_dict.get(doi, {})
                    # ç”ŸæˆåŸºäºå…ƒæ•°æ®çš„ç®€è¦æ€»ç»“
                    metadata_summary = self._generate_metadata_summary(doi, metadata, query_description)
                    if metadata_summary:
                        summary_texts.append(metadata_summary)
            
            if not summary_texts:
                logging.warning("æ— æ³•ç”Ÿæˆä»»ä½•æ–‡çŒ®æ€»ç»“æˆ–å…ƒæ•°æ®æ¡ç›®")
                return {
                    "summaries": [],
                    "code": StatusCode.NO_LITERATURE
                }
            
            total_time = (datetime.now() - query_start).total_seconds()
            logging.info(f"æŸ¥è¯¢å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}s")
            logging.info(f"è¿”å› {len(summary_texts)} ä¸ªæ¡ç›®ï¼ˆ{len(dois_with_fulltext)} ç¯‡æœ‰å…¨æ–‡çš„æ–‡çŒ®æ€»ç»“ + {len(dois_without_fulltext)} ç¯‡æ— å…¨æ–‡çš„å…ƒæ•°æ®æ¡ç›®ï¼‰")
            
            return {
                "summaries": summary_texts,
                "code": StatusCode.SUCCESS
            }
            
        except Exception as e:
            logging.error(f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}", exc_info=True)
            return {
                "summaries": [],
                "code": StatusCode.OTHER_ERROR
            }


# === MCP SERVER ===
args = parse_args()
logging.basicConfig(level=args.log_level)
logger = logging.getLogger(__name__)

# åˆå§‹åŒ–ç»“æ„åŒ–æ£€ç´¢ç³»ç»Ÿ
search_system = StructuredSearchSystem()

mcp = CalculationMCPServer("SSEkb", port=args.port, host=args.host)


# === MCP TOOL ===
@mcp.tool()
async def query_ssekb_literature(
    query_description: str
) -> QueryResult:
    """
    ğŸ“š æŸ¥è¯¢SSEkbæ–‡çŒ®çŸ¥è¯†åº“ï¼ŒåŸºäºç»“æ„åŒ–æ•°æ®åº“æ£€ç´¢è¿›è¡Œæ–‡çŒ®æ£€ç´¢å’Œæ€»ç»“ç”Ÿæˆã€‚

    ğŸ” åŠŸèƒ½è¯´æ˜:
    -----------------------------------
    æœ¬å·¥å…·é‡‡ç”¨å¤šé˜¶æ®µå¤„ç†æµç¨‹ï¼š
    1. ä½¿ç”¨LLMå°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºç»“æ„åŒ–æ•°æ®åº“æŸ¥è¯¢æ¡ä»¶ï¼ˆfiltersï¼‰
    2. æ‰§è¡Œç»“æ„åŒ–æ•°æ®åº“æŸ¥è¯¢ï¼Œä»SSEkbæ•°æ®åº“ä¸­æ£€ç´¢åŒ¹é…çš„è®ºæ–‡DOI
    3. æ£€æŸ¥å“ªäº›è®ºæ–‡æœ‰å…¨æ–‡å¯ç”¨
    4. å¯¹æœ‰å…¨æ–‡çš„è®ºæ–‡è¿›è¡Œdeep researchï¼ˆå¹¶è¡Œè¯»å–å…¨æ–‡å¹¶ç”Ÿæˆè¯¦ç»†æ€»ç»“ï¼‰
    5. å¯¹æ— å…¨æ–‡çš„è®ºæ–‡ä½¿ç”¨æ•°æ®åº“å…ƒæ•°æ®ç”Ÿæˆç®€è¦æ¡ç›®
    6. è¿”å›æ–‡çŒ®æ€»ç»“åˆ—è¡¨ï¼ˆæ±‡æ€»ç”±agentå®Œæˆï¼‰

    ğŸ§© å‚æ•°:
    -----------------------------------
    query_description : str
        è¦æŸ¥è¯¢çš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œä¾‹å¦‚ï¼š
        - "æŸ¥æ‰¾å…·æœ‰ç‰¹å®šæ€§èƒ½çš„ææ–™ç›¸å…³è®ºæ–‡"
        - "æŸ¥æ‰¾ç‰¹å®šææ–™ç±»å‹çš„ç›¸å…³è®ºæ–‡"

    ğŸ“¤ è¿”å›:
    -----------------------------------
    QueryResult (dict) åŒ…å«:
        - summaries: æ–‡çŒ®æ€»ç»“æ–‡æœ¬åˆ—è¡¨ï¼ˆList[str]ï¼‰ï¼ŒåŒ…å«è¯¦ç»†æ€»ç»“å’Œå…ƒæ•°æ®æ¡ç›®
        - code: çŠ¶æ€ç ï¼ˆä½¿ç”¨StatusCodeå¸¸é‡ï¼‰
            - StatusCode.SUCCESS (0): æ­£å¸¸å®Œæˆ
            - StatusCode.NO_RESULTS (1): æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯
            - StatusCode.NO_LITERATURE (2): æ— æ³•è¯»å–æ–‡çŒ®å…¨æ–‡
            - StatusCode.OTHER_ERROR (4): å…¶ä»–å¼‚å¸¸

    ğŸ“ ä½¿ç”¨ç¤ºä¾‹:
    -----------------------------------
    # åŸºç¡€æŸ¥è¯¢
    query_ssekb_literature(
        query_description="æŸ¥æ‰¾å…·æœ‰ç‰¹å®šæ€§èƒ½çš„ææ–™ç›¸å…³è®ºæ–‡"
    )
    """
    logger.info(f"æ”¶åˆ°æŸ¥è¯¢: {query_description}")
    
    # æ‰§è¡ŒæŸ¥è¯¢ï¼ˆåœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œï¼Œé¿å…é˜»å¡ï¼‰
    result = await to_thread.run_sync(
        lambda: search_system.query(query_description)
    )
    
    return result


# === START SERVER ===
if __name__ == "__main__":
    logger.info("Starting SSEkb MCP Server...")
    logger.info(f"Server will run on {args.host}:{args.port}")
    logger.info("Structured search system ready")
    mcp.run(transport="sse")

